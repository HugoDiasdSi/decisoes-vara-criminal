"""
Aplicativo Hugging Face Spaces - Assessor Jur√≠dico Criminal
Sistema RAG para elabora√ß√£o de decis√µes judiciais baseado em banco de conhecimento
"""

import gradio as gr
import os
from pathlib import Path
import json
from typing import List, Dict, Tuple
import google.generativeai as genai
from utils.pdf_extractor import extract_text_from_pdf
from utils.rag_system import RAGSystem
from utils.prompt_builder import build_full_prompt
from utils.rate_limiter import RateLimiter, retry_with_exponential_backoff

# Configura√ß√£o da API do Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

# Inicializar sistema RAG
rag_system = RAGSystem()

# Inicializar rate limiter (Gemini 2.0 Flash tier free: 15 RPM e 1M TPM)
# Sendo conservador com 125 TPM como mencionado pelo usu√°rio
rate_limiter = RateLimiter(tokens_per_minute=125)

def load_knowledge_base():
    """Carrega todas as minutas e metadados no sistema RAG"""
    try:
        # Carregar metadados
        with open("minutas_metadata.json", "r", encoding="utf-8") as f:
            metadata = json.load(f)

        # Carregar todas as minutas
        minutas_dir = Path(".")
        for item in metadata:
            filename = item["filename"]
            filepath = minutas_dir / filename

            if filepath.exists():
                with open(filepath, "r", encoding="utf-8") as f:
                    content = f.read()

                # Adicionar ao sistema RAG
                rag_system.add_document(
                    filename=filename,
                    content=content,
                    tags=item.get("tags", []),
                    description=item.get("descricao", "")
                )

        return True, f"Base de conhecimento carregada: {len(metadata)} minutas"
    except Exception as e:
        return False, f"Erro ao carregar base de conhecimento: {str(e)}"

def process_case(pdf_files, task_description, user_context=""):
    """
    Processa os autos digitais e gera a decis√£o judicial

    Args:
        pdf_files: Lista de arquivos PDF dos autos
        task_description: Descri√ß√£o da tarefa (ex: "elaborar decis√£o", "analisar den√∫ncia")
        user_context: Contexto adicional fornecido pelo usu√°rio
    """
    try:
        if not GEMINI_API_KEY:
            return "‚ö†Ô∏è API Key do Gemini n√£o configurada. Configure a vari√°vel de ambiente GEMINI_API_KEY."

        # ETAPA 1: Extrair texto dos PDFs
        extracted_texts = []
        status_msg = "üìÑ Extraindo texto dos PDFs...\n"

        if pdf_files:
            for pdf_file in pdf_files:
                text = extract_text_from_pdf(pdf_file.name)
                extracted_texts.append({
                    "filename": os.path.basename(pdf_file.name),
                    "content": text
                })
                status_msg += f"‚úì {os.path.basename(pdf_file.name)}\n"
        else:
            return "‚ö†Ô∏è Nenhum arquivo PDF foi fornecido."

        # ETAPA 2: An√°lise inicial e busca de minutas relevantes
        status_msg += "\nüîç Analisando autos e buscando minutas relevantes...\n"

        # Concatenar todo o conte√∫do extra√≠do
        full_text = "\n\n---\n\n".join([
            f"### {doc['filename']}\n{doc['content']}"
            for doc in extracted_texts
        ])

        # Buscar minutas relevantes usando RAG
        relevant_minutas = rag_system.search_relevant_minutas(
            query=f"{task_description} {user_context}",
            top_k=3
        )

        status_msg += f"‚úì {len(relevant_minutas)} minutas relevantes encontradas\n"
        for i, minuta in enumerate(relevant_minutas, 1):
            status_msg += f"  {i}. {minuta['filename']}\n"

        # ETAPA 3: Construir prompt completo
        status_msg += "\n‚öôÔ∏è Preparando an√°lise jur√≠dica...\n"

        full_prompt = build_full_prompt(
            autos_text=full_text,
            task=task_description,
            context=user_context,
            relevant_minutas=relevant_minutas
        )

        # ETAPA 4: Processar com Gemini
        status_msg += "\nü§ñ Processando com IA (isso pode levar alguns minutos)...\n"

        # Estimar tokens e aplicar rate limiting
        estimated_tokens = rate_limiter.estimate_tokens(full_prompt)
        status_msg += f"üìä Tokens estimados: ~{estimated_tokens}\n"

        # Verificar se excede o limite
        if estimated_tokens > 30000:  # Gemini 2.0 Flash aceita at√© 1M tokens, mas vamos ser conservadores
            status_msg += "‚ö†Ô∏è Prompt muito grande, otimizando...\n"
            # Reduzir conte√∫do das minutas se necess√°rio
            for minuta in relevant_minutas:
                if len(minuta['content']) > 3000:
                    minuta['content'] = minuta['content'][:3000] + "\n\n[... conte√∫do truncado ...]"

            # Reconstruir prompt
            full_prompt = build_full_prompt(
                autos_text=full_text[:15000] if len(full_text) > 15000 else full_text,
                task=task_description,
                context=user_context,
                relevant_minutas=relevant_minutas
            )
            estimated_tokens = rate_limiter.estimate_tokens(full_prompt)
            status_msg += f"‚úì Prompt otimizado: ~{estimated_tokens} tokens\n"

        # Aplicar rate limiting
        rate_limiter.wait_if_needed(estimated_tokens)

        # Usar Gemini 2.0 Flash (tier free)
        model = genai.GenerativeModel('gemini-2.0-flash-exp')

        # Configura√ß√£o otimizada para tier free
        generation_config = {
            "temperature": 0.3,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 8192,
        }

        # Fun√ß√£o com retry
        @retry_with_exponential_backoff
        def generate_with_retry():
            return model.generate_content(
                full_prompt,
                generation_config=generation_config
            )

        # Gerar resposta com retry autom√°tico
        response = generate_with_retry()

        # ETAPA 5: Formatar resposta
        result = f"""
{status_msg}

{'='*80}
RESULTADO DA AN√ÅLISE
{'='*80}

{response.text}

{'='*80}
MINUTAS CONSULTADAS
{'='*80}

"""
        for i, minuta in enumerate(relevant_minutas, 1):
            result += f"\n{i}. **{minuta['filename']}**\n"
            result += f"   {minuta['description']}\n"

        return result

    except Exception as e:
        return f"‚ùå Erro ao processar: {str(e)}\n\nDetalhes t√©cnicos: {type(e).__name__}"

def create_interface():
    """Cria a interface Gradio"""

    with gr.Blocks(
        title="Assessor Jur√≠dico Criminal - IA",
        theme=gr.themes.Soft()
    ) as demo:

        gr.Markdown("""
        # ‚öñÔ∏è Assessor Jur√≠dico Criminal - Sistema RAG

        Sistema de intelig√™ncia artificial para apoio √† elabora√ß√£o de decis√µes judiciais criminais,
        baseado em banco de conhecimento com minutas de decis√µes anteriores.

        ### üìã Como usar:
        1. Fa√ßa upload dos PDFs dos autos processuais
        2. Descreva a tarefa que deseja realizar
        3. Adicione contexto adicional se necess√°rio
        4. Clique em "Processar" e aguarde a an√°lise

        ‚è±Ô∏è **Tier Free**: Este app usa Gemini 2.0 Flash tier gratuito (125 TPM).
        Pode haver tempo de espera entre requisi√ß√µes para respeitar os limites da API.
        """)

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### üìé Upload de Autos")
                pdf_input = gr.File(
                    label="Autos Digitais (PDFs)",
                    file_count="multiple",
                    file_types=[".pdf"],
                    type="filepath"
                )

                gr.Markdown("### üìù Tarefa")
                task_input = gr.Textbox(
                    label="Descri√ß√£o da tarefa",
                    placeholder="Ex: Elaborar decis√£o de recebimento da den√∫ncia\nAnalise preliminar para absolvi√ß√£o sum√°ria\nDecis√£o sobre pedido de liberdade provis√≥ria",
                    lines=3
                )

                context_input = gr.Textbox(
                    label="Contexto adicional (opcional)",
                    placeholder="Informa√ß√µes adicionais sobre o caso, peculiaridades, observa√ß√µes...",
                    lines=3
                )

                process_btn = gr.Button(
                    "üöÄ Processar",
                    variant="primary",
                    size="lg"
                )

            with gr.Column(scale=2):
                gr.Markdown("### üìÑ Resultado")
                output = gr.Textbox(
                    label="An√°lise e Decis√£o",
                    lines=30,
                    max_lines=50,
                    show_copy_button=True
                )

        gr.Markdown("""
        ---
        ### ‚ÑπÔ∏è Informa√ß√µes Importantes

        - **Sistema RAG**: Utiliza Retrieval-Augmented Generation para buscar minutas relevantes
        - **Base de conhecimento**: Mais de 70 minutas de decis√µes judiciais
        - **Modelo de IA**: Google Gemini 2.0 Flash (Tier Free - 125 TPM)
        - **OCR**: Suporta PDFs digitalizados e nativos
        - **Rate Limiting**: Sistema autom√°tico de controle de taxa para respeitar limites da API

        ‚ö†Ô∏è **Aviso Legal**: Este sistema √© uma ferramenta de apoio. Todas as decis√µes devem ser revisadas
        por um magistrado antes de serem publicadas.

        ‚è≥ **Tempo de Processamento**: Devido aos limites do tier gratuito, o processamento pode levar
        alguns minutos, especialmente para PDFs grandes ou m√∫ltiplos autos.

        üìö **Base de Conhecimento**: [GitHub Repository](https://github.com/HugoDiasdSi/decisoes-vara-criminal)
        """)

        # Event handlers
        process_btn.click(
            fn=process_case,
            inputs=[pdf_input, task_input, context_input],
            outputs=output
        )

        # Carregar base de conhecimento ao iniciar
        demo.load(
            fn=load_knowledge_base,
            outputs=None
        )

    return demo

# Inicializar e lan√ßar aplicativo
if __name__ == "__main__":
    # Carregar base de conhecimento
    success, message = load_knowledge_base()
    print(message)

    # Criar e lan√ßar interface
    demo = create_interface()
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )
